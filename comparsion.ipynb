{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "def evaluate_lstm(model_path, X_test, y_test, coin_column='coin'):\n",
    "    \"\"\"\n",
    "    Evaluate LSTM model with Embedding layer for coin IDs\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved LSTM model (.pkl)\n",
    "        X_test: Test features (must include 'coin' column)\n",
    "        y_test: Test labels\n",
    "        coin_column: Name of column containing coin IDs\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metrics and predictions\n",
    "    \"\"\"\n",
    "    # Load model and associated components\n",
    "    model_data = joblib.load(model_path)\n",
    "    lstm_model = model_data['model']\n",
    "    coin_encoder = model_data['coin_encoder']  # LabelEncoder for coins\n",
    "    feature_scaler = model_data['feature_scaler']  # Scaler for other features\n",
    "    seq_length = model_data.get('seq_length', 10)  # Default 10 if not stored\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    # 1. Encode coins\n",
    "    X_test['coin_code'] = coin_encoder.transform(X_test[coin_column])\n",
    "    \n",
    "    # 2. Scale numerical features\n",
    "    num_features = ['rsi_14', 'volumeto', 'macd_line']  # Update with your actual features\n",
    "    X_test[num_features] = feature_scaler.transform(X_test[num_features])\n",
    "    \n",
    "    # 3. Create sequences\n",
    "    test_gen = TimeseriesGenerator(\n",
    "        data=X_test[['coin_code'] + num_features].values,\n",
    "        targets=y_test.values,\n",
    "        length=seq_length,\n",
    "        batch_size=len(X_test)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_proba = lstm_model.predict(test_gen[0][0]).flatten()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    return {\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_proba,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have:\n",
    "    # - X_test (DataFrame with 'coin' column and features)\n",
    "    # - y_test (labels)\n",
    "    \n",
    "    lstm_results = evaluate_lstm(\n",
    "        model_path='models/lstm_model.pkl',\n",
    "        X_test=X_test,\n",
    "        y_test=y_test\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM Performance Metrics:\")\n",
    "    print(pd.DataFrame([lstm_results['metrics']]))\n",
    "    \n",
    "    # To compare with other models:\n",
    "    model_paths = {\n",
    "        'Logistic Regression': 'models/lr_model.pkl',\n",
    "        'Decision Tree': 'models/dt_model.pkl',\n",
    "        'Random Forest': 'models/rf_model.pkl',\n",
    "        'XGBoost': 'models/xgb_model.pkl',\n",
    "        'LSTM': 'models/lstm_model.pkl'\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    for name, path in model_paths.items():\n",
    "        if name == 'LSTM':\n",
    "            all_results[name] = evaluate_lstm(path, X_test, y_test)['metrics']\n",
    "        else:\n",
    "            model = joblib.load(path)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred)\n",
    "            }\n",
    "            if y_proba is not None:\n",
    "                metrics['auc'] = roc_auc_score(y_test, y_proba)\n",
    "            \n",
    "            all_results[name] = metrics\n",
    "    \n",
    "    # Create comparison table\n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(results_df.sort_values('auc', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
